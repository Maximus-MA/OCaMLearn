<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>tensor_tests.ml &mdash; Coverage report</title>
    <meta name="description" content="93.09% coverage in src/test/tensor_tests.ml">
    <link rel="stylesheet" href="../../coverage.css"/>
    <script src="../../highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body>
    <div id="header">
      <h1>
        <a href="../../index.html">
          <span class="dirname">src/test/</span>tensor_tests.ml
        </a>
      </h1>
      <h2>93.09%</h2>
    </div>
    <div id="navbar">
      <span class="some-visited" style="top:1.83%"></span>
      <span class="unvisited" style="top:9.16%"></span>
      <span class="unvisited" style="top:15.75%"></span>
      <span class="unvisited" style="top:23.63%"></span>
      <span class="unvisited" style="top:30.22%"></span>
      <span class="some-visited" style="top:45.42%"></span>
      <span class="some-visited" style="top:45.97%"></span>
      <span class="some-visited" style="bottom:46.89%"></span>
      <span class="some-visited" style="bottom:35.16%"></span>
      <span class="some-visited" style="bottom:32.78%"></span>
      <span class="some-visited" style="bottom:30.22%"></span>
      <span class="some-visited" style="bottom:26.74%"></span>
      <span class="some-visited" style="bottom:23.99%"></span>
      <span class="some-visited" style="bottom:21.61%"></span>
      <span class="some-visited" style="bottom:19.78%"></span>
      <span class="unvisited" style="bottom:14.29%"></span>
      <span class="unvisited" style="bottom:9.34%"></span>
    </div>
    <div id="report">
      <div id="lines-layer">
        <pre>
<a id="L1"></a><span > </span>
<a id="L2"></a><span > </span>
<a id="L3"></a><span > </span>
<a id="L4"></a><span > </span>
<a id="L5"></a><span > </span>
<a id="L6"></a><span > </span>
<a id="L7"></a><span > </span>
<a id="L8"></a><span class="visited"> </span>
<a id="L9"></a><span class="visited"> </span>
<a id="L10"></a><span class="some-visited"> </span>
<a id="L11"></a><span class="visited"> </span>
<a id="L12"></a><span class="visited"> </span>
<a id="L13"></a><span > </span>
<a id="L14"></a><span > </span>
<a id="L15"></a><span class="visited"> </span>
<a id="L16"></a><span class="visited"> </span>
<a id="L17"></a><span class="visited"> </span>
<a id="L18"></a><span > </span>
<a id="L19"></a><span > </span>
<a id="L20"></a><span class="visited"> </span>
<a id="L21"></a><span class="visited"> </span>
<a id="L22"></a><span class="visited"> </span>
<a id="L23"></a><span > </span>
<a id="L24"></a><span > </span>
<a id="L25"></a><span > </span>
<a id="L26"></a><span > </span>
<a id="L27"></a><span > </span>
<a id="L28"></a><span > </span>
<a id="L29"></a><span > </span>
<a id="L30"></a><span > </span>
<a id="L31"></a><span > </span>
<a id="L32"></a><span > </span>
<a id="L33"></a><span > </span>
<a id="L34"></a><span > </span>
<a id="L35"></a><span > </span>
<a id="L36"></a><span > </span>
<a id="L37"></a><span > </span>
<a id="L38"></a><span class="visited"> </span>
<a id="L39"></a><span class="visited"> </span>
<a id="L40"></a><span class="visited"> </span>
<a id="L41"></a><span class="visited"> </span>
<a id="L42"></a><span class="visited"> </span>
<a id="L43"></a><span class="visited"> </span>
<a id="L44"></a><span > </span>
<a id="L45"></a><span > </span>
<a id="L46"></a><span class="visited"> </span>
<a id="L47"></a><span class="visited"> </span>
<a id="L48"></a><span > </span>
<a id="L49"></a><span class="visited"> </span>
<a id="L50"></a><span class="unvisited"> </span>
<a id="L51"></a><span > </span>
<a id="L52"></a><span > </span>
<a id="L53"></a><span > </span>
<a id="L54"></a><span > </span>
<a id="L55"></a><span > </span>
<a id="L56"></a><span > </span>
<a id="L57"></a><span > </span>
<a id="L58"></a><span > </span>
<a id="L59"></a><span class="visited"> </span>
<a id="L60"></a><span class="visited"> </span>
<a id="L61"></a><span class="visited"> </span>
<a id="L62"></a><span > </span>
<a id="L63"></a><span > </span>
<a id="L64"></a><span > </span>
<a id="L65"></a><span > </span>
<a id="L66"></a><span > </span>
<a id="L67"></a><span > </span>
<a id="L68"></a><span > </span>
<a id="L69"></a><span > </span>
<a id="L70"></a><span > </span>
<a id="L71"></a><span > </span>
<a id="L72"></a><span > </span>
<a id="L73"></a><span > </span>
<a id="L74"></a><span class="visited"> </span>
<a id="L75"></a><span class="visited"> </span>
<a id="L76"></a><span class="visited"> </span>
<a id="L77"></a><span class="visited"> </span>
<a id="L78"></a><span class="visited"> </span>
<a id="L79"></a><span class="visited"> </span>
<a id="L80"></a><span > </span>
<a id="L81"></a><span > </span>
<a id="L82"></a><span class="visited"> </span>
<a id="L83"></a><span class="visited"> </span>
<a id="L84"></a><span > </span>
<a id="L85"></a><span class="visited"> </span>
<a id="L86"></a><span class="unvisited"> </span>
<a id="L87"></a><span > </span>
<a id="L88"></a><span > </span>
<a id="L89"></a><span > </span>
<a id="L90"></a><span > </span>
<a id="L91"></a><span > </span>
<a id="L92"></a><span > </span>
<a id="L93"></a><span > </span>
<a id="L94"></a><span > </span>
<a id="L95"></a><span > </span>
<a id="L96"></a><span > </span>
<a id="L97"></a><span > </span>
<a id="L98"></a><span > </span>
<a id="L99"></a><span > </span>
<a id="L100"></a><span > </span>
<a id="L101"></a><span > </span>
<a id="L102"></a><span class="visited"> </span>
<a id="L103"></a><span > </span>
<a id="L104"></a><span class="visited"> </span>
<a id="L105"></a><span class="visited"> </span>
<a id="L106"></a><span class="visited"> </span>
<a id="L107"></a><span class="visited"> </span>
<a id="L108"></a><span > </span>
<a id="L109"></a><span > </span>
<a id="L110"></a><span > </span>
<a id="L111"></a><span > </span>
<a id="L112"></a><span > </span>
<a id="L113"></a><span > </span>
<a id="L114"></a><span > </span>
<a id="L115"></a><span > </span>
<a id="L116"></a><span > </span>
<a id="L117"></a><span class="visited"> </span>
<a id="L118"></a><span class="visited"> </span>
<a id="L119"></a><span class="visited"> </span>
<a id="L120"></a><span class="visited"> </span>
<a id="L121"></a><span class="visited"> </span>
<a id="L122"></a><span class="visited"> </span>
<a id="L123"></a><span > </span>
<a id="L124"></a><span > </span>
<a id="L125"></a><span class="visited"> </span>
<a id="L126"></a><span class="visited"> </span>
<a id="L127"></a><span > </span>
<a id="L128"></a><span class="visited"> </span>
<a id="L129"></a><span class="unvisited"> </span>
<a id="L130"></a><span > </span>
<a id="L131"></a><span > </span>
<a id="L132"></a><span > </span>
<a id="L133"></a><span > </span>
<a id="L134"></a><span > </span>
<a id="L135"></a><span > </span>
<a id="L136"></a><span > </span>
<a id="L137"></a><span > </span>
<a id="L138"></a><span class="visited"> </span>
<a id="L139"></a><span class="visited"> </span>
<a id="L140"></a><span class="visited"> </span>
<a id="L141"></a><span class="visited"> </span>
<a id="L142"></a><span class="visited"> </span>
<a id="L143"></a><span > </span>
<a id="L144"></a><span > </span>
<a id="L145"></a><span > </span>
<a id="L146"></a><span > </span>
<a id="L147"></a><span > </span>
<a id="L148"></a><span > </span>
<a id="L149"></a><span > </span>
<a id="L150"></a><span > </span>
<a id="L151"></a><span > </span>
<a id="L152"></a><span > </span>
<a id="L153"></a><span class="visited"> </span>
<a id="L154"></a><span class="visited"> </span>
<a id="L155"></a><span class="visited"> </span>
<a id="L156"></a><span class="visited"> </span>
<a id="L157"></a><span class="visited"> </span>
<a id="L158"></a><span class="visited"> </span>
<a id="L159"></a><span > </span>
<a id="L160"></a><span > </span>
<a id="L161"></a><span class="visited"> </span>
<a id="L162"></a><span class="visited"> </span>
<a id="L163"></a><span > </span>
<a id="L164"></a><span class="visited"> </span>
<a id="L165"></a><span class="unvisited"> </span>
<a id="L166"></a><span > </span>
<a id="L167"></a><span > </span>
<a id="L168"></a><span > </span>
<a id="L169"></a><span > </span>
<a id="L170"></a><span > </span>
<a id="L171"></a><span > </span>
<a id="L172"></a><span > </span>
<a id="L173"></a><span > </span>
<a id="L174"></a><span > </span>
<a id="L175"></a><span > </span>
<a id="L176"></a><span > </span>
<a id="L177"></a><span > </span>
<a id="L178"></a><span > </span>
<a id="L179"></a><span > </span>
<a id="L180"></a><span > </span>
<a id="L181"></a><span > </span>
<a id="L182"></a><span > </span>
<a id="L183"></a><span > </span>
<a id="L184"></a><span > </span>
<a id="L185"></a><span class="visited"> </span>
<a id="L186"></a><span class="visited"> </span>
<a id="L187"></a><span class="visited"> </span>
<a id="L188"></a><span class="visited"> </span>
<a id="L189"></a><span class="visited"> </span>
<a id="L190"></a><span > </span>
<a id="L191"></a><span > </span>
<a id="L192"></a><span > </span>
<a id="L193"></a><span > </span>
<a id="L194"></a><span class="visited"> </span>
<a id="L195"></a><span class="visited"> </span>
<a id="L196"></a><span class="visited"> </span>
<a id="L197"></a><span class="visited"> </span>
<a id="L198"></a><span class="visited"> </span>
<a id="L199"></a><span > </span>
<a id="L200"></a><span > </span>
<a id="L201"></a><span class="visited"> </span>
<a id="L202"></a><span class="visited"> </span>
<a id="L203"></a><span class="visited"> </span>
<a id="L204"></a><span > </span>
<a id="L205"></a><span > </span>
<a id="L206"></a><span class="visited"> </span>
<a id="L207"></a><span class="visited"> </span>
<a id="L208"></a><span class="visited"> </span>
<a id="L209"></a><span > </span>
<a id="L210"></a><span > </span>
<a id="L211"></a><span class="visited"> </span>
<a id="L212"></a><span class="visited"> </span>
<a id="L213"></a><span class="visited"> </span>
<a id="L214"></a><span > </span>
<a id="L215"></a><span > </span>
<a id="L216"></a><span class="visited"> </span>
<a id="L217"></a><span > </span>
<a id="L218"></a><span class="visited"> </span>
<a id="L219"></a><span > </span>
<a id="L220"></a><span class="visited"> </span>
<a id="L221"></a><span class="visited"> </span>
<a id="L222"></a><span > </span>
<a id="L223"></a><span > </span>
<a id="L224"></a><span class="visited"> </span>
<a id="L225"></a><span > </span>
<a id="L226"></a><span class="visited"> </span>
<a id="L227"></a><span > </span>
<a id="L228"></a><span > </span>
<a id="L229"></a><span class="visited"> </span>
<a id="L230"></a><span > </span>
<a id="L231"></a><span class="visited"> </span>
<a id="L232"></a><span > </span>
<a id="L233"></a><span > </span>
<a id="L234"></a><span > </span>
<a id="L235"></a><span class="visited"> </span>
<a id="L236"></a><span class="visited"> </span>
<a id="L237"></a><span class="visited"> </span>
<a id="L238"></a><span class="visited"> </span>
<a id="L239"></a><span class="visited"> </span>
<a id="L240"></a><span > </span>
<a id="L241"></a><span > </span>
<a id="L242"></a><span > </span>
<a id="L243"></a><span class="visited"> </span>
<a id="L244"></a><span class="visited"> </span>
<a id="L245"></a><span class="visited"> </span>
<a id="L246"></a><span class="visited"> </span>
<a id="L247"></a><span class="visited"> </span>
<a id="L248"></a><span class="some-visited"> </span>
<a id="L249"></a><span > </span>
<a id="L250"></a><span class="visited"> </span>
<a id="L251"></a><span class="some-visited"> </span>
<a id="L252"></a><span > </span>
<a id="L253"></a><span > </span>
<a id="L254"></a><span class="visited"> </span>
<a id="L255"></a><span class="visited"> </span>
<a id="L256"></a><span class="visited"> </span>
<a id="L257"></a><span > </span>
<a id="L258"></a><span > </span>
<a id="L259"></a><span > </span>
<a id="L260"></a><span > </span>
<a id="L261"></a><span > </span>
<a id="L262"></a><span > </span>
<a id="L263"></a><span > </span>
<a id="L264"></a><span > </span>
<a id="L265"></a><span > </span>
<a id="L266"></a><span > </span>
<a id="L267"></a><span > </span>
<a id="L268"></a><span > </span>
<a id="L269"></a><span > </span>
<a id="L270"></a><span > </span>
<a id="L271"></a><span > </span>
<a id="L272"></a><span > </span>
<a id="L273"></a><span class="visited"> </span>
<a id="L274"></a><span class="visited"> </span>
<a id="L275"></a><span > </span>
<a id="L276"></a><span class="visited"> </span>
<a id="L277"></a><span > </span>
<a id="L278"></a><span > </span>
<a id="L279"></a><span > </span>
<a id="L280"></a><span > </span>
<a id="L281"></a><span > </span>
<a id="L282"></a><span > </span>
<a id="L283"></a><span > </span>
<a id="L284"></a><span > </span>
<a id="L285"></a><span > </span>
<a id="L286"></a><span > </span>
<a id="L287"></a><span class="visited"> </span>
<a id="L288"></a><span class="visited"> </span>
<a id="L289"></a><span class="visited"> </span>
<a id="L290"></a><span class="some-visited"> </span>
<a id="L291"></a><span > </span>
<a id="L292"></a><span > </span>
<a id="L293"></a><span > </span>
<a id="L294"></a><span > </span>
<a id="L295"></a><span > </span>
<a id="L296"></a><span > </span>
<a id="L297"></a><span > </span>
<a id="L298"></a><span > </span>
<a id="L299"></a><span > </span>
<a id="L300"></a><span > </span>
<a id="L301"></a><span > </span>
<a id="L302"></a><span > </span>
<a id="L303"></a><span > </span>
<a id="L304"></a><span > </span>
<a id="L305"></a><span > </span>
<a id="L306"></a><span > </span>
<a id="L307"></a><span > </span>
<a id="L308"></a><span > </span>
<a id="L309"></a><span > </span>
<a id="L310"></a><span > </span>
<a id="L311"></a><span > </span>
<a id="L312"></a><span > </span>
<a id="L313"></a><span > </span>
<a id="L314"></a><span > </span>
<a id="L315"></a><span > </span>
<a id="L316"></a><span > </span>
<a id="L317"></a><span > </span>
<a id="L318"></a><span > </span>
<a id="L319"></a><span > </span>
<a id="L320"></a><span > </span>
<a id="L321"></a><span > </span>
<a id="L322"></a><span > </span>
<a id="L323"></a><span > </span>
<a id="L324"></a><span > </span>
<a id="L325"></a><span > </span>
<a id="L326"></a><span > </span>
<a id="L327"></a><span > </span>
<a id="L328"></a><span class="visited"> </span>
<a id="L329"></a><span class="visited"> </span>
<a id="L330"></a><span class="visited"> </span>
<a id="L331"></a><span > </span>
<a id="L332"></a><span > </span>
<a id="L333"></a><span > </span>
<a id="L334"></a><span > </span>
<a id="L335"></a><span > </span>
<a id="L336"></a><span > </span>
<a id="L337"></a><span > </span>
<a id="L338"></a><span > </span>
<a id="L339"></a><span > </span>
<a id="L340"></a><span > </span>
<a id="L341"></a><span > </span>
<a id="L342"></a><span > </span>
<a id="L343"></a><span > </span>
<a id="L344"></a><span > </span>
<a id="L345"></a><span > </span>
<a id="L346"></a><span > </span>
<a id="L347"></a><span > </span>
<a id="L348"></a><span > </span>
<a id="L349"></a><span > </span>
<a id="L350"></a><span class="visited"> </span>
<a id="L351"></a><span class="visited"> </span>
<a id="L352"></a><span class="visited"> </span>
<a id="L353"></a><span class="visited"> </span>
<a id="L354"></a><span class="some-visited"> </span>
<a id="L355"></a><span > </span>
<a id="L356"></a><span > </span>
<a id="L357"></a><span class="visited"> </span>
<a id="L358"></a><span > </span>
<a id="L359"></a><span > </span>
<a id="L360"></a><span > </span>
<a id="L361"></a><span class="visited"> </span>
<a id="L362"></a><span class="visited"> </span>
<a id="L363"></a><span > </span>
<a id="L364"></a><span class="visited"> </span>
<a id="L365"></a><span class="visited"> </span>
<a id="L366"></a><span class="visited"> </span>
<a id="L367"></a><span class="some-visited"> </span>
<a id="L368"></a><span > </span>
<a id="L369"></a><span > </span>
<a id="L370"></a><span class="visited"> </span>
<a id="L371"></a><span > </span>
<a id="L372"></a><span > </span>
<a id="L373"></a><span > </span>
<a id="L374"></a><span class="visited"> </span>
<a id="L375"></a><span > </span>
<a id="L376"></a><span class="visited"> </span>
<a id="L377"></a><span > </span>
<a id="L378"></a><span class="visited"> </span>
<a id="L379"></a><span class="visited"> </span>
<a id="L380"></a><span class="visited"> </span>
<a id="L381"></a><span class="some-visited"> </span>
<a id="L382"></a><span > </span>
<a id="L383"></a><span > </span>
<a id="L384"></a><span > </span>
<a id="L385"></a><span > </span>
<a id="L386"></a><span > </span>
<a id="L387"></a><span > </span>
<a id="L388"></a><span > </span>
<a id="L389"></a><span > </span>
<a id="L390"></a><span class="visited"> </span>
<a id="L391"></a><span > </span>
<a id="L392"></a><span > </span>
<a id="L393"></a><span > </span>
<a id="L394"></a><span class="visited"> </span>
<a id="L395"></a><span class="visited"> </span>
<a id="L396"></a><span > </span>
<a id="L397"></a><span class="visited"> </span>
<a id="L398"></a><span class="visited"> </span>
<a id="L399"></a><span class="visited"> </span>
<a id="L400"></a><span class="some-visited"> </span>
<a id="L401"></a><span > </span>
<a id="L402"></a><span > </span>
<a id="L403"></a><span > </span>
<a id="L404"></a><span > </span>
<a id="L405"></a><span > </span>
<a id="L406"></a><span class="visited"> </span>
<a id="L407"></a><span > </span>
<a id="L408"></a><span > </span>
<a id="L409"></a><span > </span>
<a id="L410"></a><span class="visited"> </span>
<a id="L411"></a><span class="visited"> </span>
<a id="L412"></a><span class="visited"> </span>
<a id="L413"></a><span class="visited"> </span>
<a id="L414"></a><span class="visited"> </span>
<a id="L415"></a><span class="some-visited"> </span>
<a id="L416"></a><span > </span>
<a id="L417"></a><span > </span>
<a id="L418"></a><span class="visited"> </span>
<a id="L419"></a><span > </span>
<a id="L420"></a><span > </span>
<a id="L421"></a><span > </span>
<a id="L422"></a><span class="visited"> </span>
<a id="L423"></a><span class="visited"> </span>
<a id="L424"></a><span class="visited"> </span>
<a id="L425"></a><span class="visited"> </span>
<a id="L426"></a><span class="visited"> </span>
<a id="L427"></a><span class="visited"> </span>
<a id="L428"></a><span class="some-visited"> </span>
<a id="L429"></a><span > </span>
<a id="L430"></a><span > </span>
<a id="L431"></a><span class="visited"> </span>
<a id="L432"></a><span class="visited"> </span>
<a id="L433"></a><span > </span>
<a id="L434"></a><span > </span>
<a id="L435"></a><span > </span>
<a id="L436"></a><span class="visited"> </span>
<a id="L437"></a><span class="visited"> </span>
<a id="L438"></a><span class="some-visited"> </span>
<a id="L439"></a><span class="visited"> </span>
<a id="L440"></a><span class="visited"> </span>
<a id="L441"></a><span > </span>
<a id="L442"></a><span > </span>
<a id="L443"></a><span class="visited"> </span>
<a id="L444"></a><span class="visited"> </span>
<a id="L445"></a><span class="visited"> </span>
<a id="L446"></a><span > </span>
<a id="L447"></a><span > </span>
<a id="L448"></a><span > </span>
<a id="L449"></a><span > </span>
<a id="L450"></a><span > </span>
<a id="L451"></a><span > </span>
<a id="L452"></a><span > </span>
<a id="L453"></a><span class="visited"> </span>
<a id="L454"></a><span class="visited"> </span>
<a id="L455"></a><span class="visited"> </span>
<a id="L456"></a><span > </span>
<a id="L457"></a><span > </span>
<a id="L458"></a><span class="visited"> </span>
<a id="L459"></a><span class="visited"> </span>
<a id="L460"></a><span class="visited"> </span>
<a id="L461"></a><span class="visited"> </span>
<a id="L462"></a><span > </span>
<a id="L463"></a><span > </span>
<a id="L464"></a><span class="visited"> </span>
<a id="L465"></a><span class="visited"> </span>
<a id="L466"></a><span > </span>
<a id="L467"></a><span class="visited"> </span>
<a id="L468"></a><span class="unvisited"> </span>
<a id="L469"></a><span > </span>
<a id="L470"></a><span > </span>
<a id="L471"></a><span class="visited"> </span>
<a id="L472"></a><span class="visited"> </span>
<a id="L473"></a><span class="visited"> </span>
<a id="L474"></a><span class="visited"> </span>
<a id="L475"></a><span class="visited"> </span>
<a id="L476"></a><span class="visited"> </span>
<a id="L477"></a><span > </span>
<a id="L478"></a><span > </span>
<a id="L479"></a><span > </span>
<a id="L480"></a><span > </span>
<a id="L481"></a><span class="visited"> </span>
<a id="L482"></a><span class="visited"> </span>
<a id="L483"></a><span class="visited"> </span>
<a id="L484"></a><span > </span>
<a id="L485"></a><span class="visited"> </span>
<a id="L486"></a><span class="visited"> </span>
<a id="L487"></a><span class="visited"> </span>
<a id="L488"></a><span class="visited"> </span>
<a id="L489"></a><span > </span>
<a id="L490"></a><span > </span>
<a id="L491"></a><span class="visited"> </span>
<a id="L492"></a><span class="visited"> </span>
<a id="L493"></a><span > </span>
<a id="L494"></a><span class="visited"> </span>
<a id="L495"></a><span class="unvisited"> </span>
<a id="L496"></a><span > </span>
<a id="L497"></a><span > </span>
<a id="L498"></a><span class="visited"> </span>
<a id="L499"></a><span class="visited"> </span>
<a id="L500"></a><span class="visited"> </span>
<a id="L501"></a><span class="visited"> </span>
<a id="L502"></a><span class="visited"> </span>
<a id="L503"></a><span class="visited"> </span>
<a id="L504"></a><span > </span>
<a id="L505"></a><span > </span>
<a id="L506"></a><span class="visited"> </span>
<a id="L507"></a><span > </span>
<a id="L508"></a><span > </span>
<a id="L509"></a><span > </span>
<a id="L510"></a><span class="visited"> </span>
<a id="L511"></a><span > </span>
<a id="L512"></a><span class="visited"> </span>
<a id="L513"></a><span > </span>
<a id="L514"></a><span > </span>
<a id="L515"></a><span class="visited"> </span>
<a id="L516"></a><span class="visited"> </span>
<a id="L517"></a><span > </span>
<a id="L518"></a><span > </span>
<a id="L519"></a><span class="visited"> </span>
<a id="L520"></a><span class="visited"> </span>
<a id="L521"></a><span class="visited"> </span>
<a id="L522"></a><span class="visited"> </span>
<a id="L523"></a><span class="visited"> </span>
<a id="L524"></a><span class="visited"> </span>
<a id="L525"></a><span class="visited"> </span>
<a id="L526"></a><span class="visited"> </span>
<a id="L527"></a><span class="visited"> </span>
<a id="L528"></a><span class="visited"> </span>
<a id="L529"></a><span class="visited"> </span>
<a id="L530"></a><span class="visited"> </span>
<a id="L531"></a><span class="visited"> </span>
<a id="L532"></a><span class="visited"> </span>
<a id="L533"></a><span class="visited"> </span>
<a id="L534"></a><span class="visited"> </span>
<a id="L535"></a><span class="visited"> </span>
<a id="L536"></a><span class="visited"> </span>
<a id="L537"></a><span class="visited"> </span>
<a id="L538"></a><span class="visited"> </span>
<a id="L539"></a><span class="visited"> </span>
<a id="L540"></a><span class="visited"> </span>
<a id="L541"></a><span class="visited"> </span>
<a id="L542"></a><span class="visited"> </span>
<a id="L543"></a><span > </span>
<a id="L544"></a><span > </span>
<a id="L545"></a><span > </span>
<a id="L546"></a><span class="visited"> </span>
</pre>
      </div>
      <div id="text-layer">
        <pre id="line-numbers">
<a href="#L1">  1</a>
<a href="#L2">  2</a>
<a href="#L3">  3</a>
<a href="#L4">  4</a>
<a href="#L5">  5</a>
<a href="#L6">  6</a>
<a href="#L7">  7</a>
<a href="#L8">  8</a>
<a href="#L9">  9</a>
<a href="#L10"> 10</a>
<a href="#L11"> 11</a>
<a href="#L12"> 12</a>
<a href="#L13"> 13</a>
<a href="#L14"> 14</a>
<a href="#L15"> 15</a>
<a href="#L16"> 16</a>
<a href="#L17"> 17</a>
<a href="#L18"> 18</a>
<a href="#L19"> 19</a>
<a href="#L20"> 20</a>
<a href="#L21"> 21</a>
<a href="#L22"> 22</a>
<a href="#L23"> 23</a>
<a href="#L24"> 24</a>
<a href="#L25"> 25</a>
<a href="#L26"> 26</a>
<a href="#L27"> 27</a>
<a href="#L28"> 28</a>
<a href="#L29"> 29</a>
<a href="#L30"> 30</a>
<a href="#L31"> 31</a>
<a href="#L32"> 32</a>
<a href="#L33"> 33</a>
<a href="#L34"> 34</a>
<a href="#L35"> 35</a>
<a href="#L36"> 36</a>
<a href="#L37"> 37</a>
<a href="#L38"> 38</a>
<a href="#L39"> 39</a>
<a href="#L40"> 40</a>
<a href="#L41"> 41</a>
<a href="#L42"> 42</a>
<a href="#L43"> 43</a>
<a href="#L44"> 44</a>
<a href="#L45"> 45</a>
<a href="#L46"> 46</a>
<a href="#L47"> 47</a>
<a href="#L48"> 48</a>
<a href="#L49"> 49</a>
<a href="#L50"> 50</a>
<a href="#L51"> 51</a>
<a href="#L52"> 52</a>
<a href="#L53"> 53</a>
<a href="#L54"> 54</a>
<a href="#L55"> 55</a>
<a href="#L56"> 56</a>
<a href="#L57"> 57</a>
<a href="#L58"> 58</a>
<a href="#L59"> 59</a>
<a href="#L60"> 60</a>
<a href="#L61"> 61</a>
<a href="#L62"> 62</a>
<a href="#L63"> 63</a>
<a href="#L64"> 64</a>
<a href="#L65"> 65</a>
<a href="#L66"> 66</a>
<a href="#L67"> 67</a>
<a href="#L68"> 68</a>
<a href="#L69"> 69</a>
<a href="#L70"> 70</a>
<a href="#L71"> 71</a>
<a href="#L72"> 72</a>
<a href="#L73"> 73</a>
<a href="#L74"> 74</a>
<a href="#L75"> 75</a>
<a href="#L76"> 76</a>
<a href="#L77"> 77</a>
<a href="#L78"> 78</a>
<a href="#L79"> 79</a>
<a href="#L80"> 80</a>
<a href="#L81"> 81</a>
<a href="#L82"> 82</a>
<a href="#L83"> 83</a>
<a href="#L84"> 84</a>
<a href="#L85"> 85</a>
<a href="#L86"> 86</a>
<a href="#L87"> 87</a>
<a href="#L88"> 88</a>
<a href="#L89"> 89</a>
<a href="#L90"> 90</a>
<a href="#L91"> 91</a>
<a href="#L92"> 92</a>
<a href="#L93"> 93</a>
<a href="#L94"> 94</a>
<a href="#L95"> 95</a>
<a href="#L96"> 96</a>
<a href="#L97"> 97</a>
<a href="#L98"> 98</a>
<a href="#L99"> 99</a>
<a href="#L100">100</a>
<a href="#L101">101</a>
<a href="#L102">102</a>
<a href="#L103">103</a>
<a href="#L104">104</a>
<a href="#L105">105</a>
<a href="#L106">106</a>
<a href="#L107">107</a>
<a href="#L108">108</a>
<a href="#L109">109</a>
<a href="#L110">110</a>
<a href="#L111">111</a>
<a href="#L112">112</a>
<a href="#L113">113</a>
<a href="#L114">114</a>
<a href="#L115">115</a>
<a href="#L116">116</a>
<a href="#L117">117</a>
<a href="#L118">118</a>
<a href="#L119">119</a>
<a href="#L120">120</a>
<a href="#L121">121</a>
<a href="#L122">122</a>
<a href="#L123">123</a>
<a href="#L124">124</a>
<a href="#L125">125</a>
<a href="#L126">126</a>
<a href="#L127">127</a>
<a href="#L128">128</a>
<a href="#L129">129</a>
<a href="#L130">130</a>
<a href="#L131">131</a>
<a href="#L132">132</a>
<a href="#L133">133</a>
<a href="#L134">134</a>
<a href="#L135">135</a>
<a href="#L136">136</a>
<a href="#L137">137</a>
<a href="#L138">138</a>
<a href="#L139">139</a>
<a href="#L140">140</a>
<a href="#L141">141</a>
<a href="#L142">142</a>
<a href="#L143">143</a>
<a href="#L144">144</a>
<a href="#L145">145</a>
<a href="#L146">146</a>
<a href="#L147">147</a>
<a href="#L148">148</a>
<a href="#L149">149</a>
<a href="#L150">150</a>
<a href="#L151">151</a>
<a href="#L152">152</a>
<a href="#L153">153</a>
<a href="#L154">154</a>
<a href="#L155">155</a>
<a href="#L156">156</a>
<a href="#L157">157</a>
<a href="#L158">158</a>
<a href="#L159">159</a>
<a href="#L160">160</a>
<a href="#L161">161</a>
<a href="#L162">162</a>
<a href="#L163">163</a>
<a href="#L164">164</a>
<a href="#L165">165</a>
<a href="#L166">166</a>
<a href="#L167">167</a>
<a href="#L168">168</a>
<a href="#L169">169</a>
<a href="#L170">170</a>
<a href="#L171">171</a>
<a href="#L172">172</a>
<a href="#L173">173</a>
<a href="#L174">174</a>
<a href="#L175">175</a>
<a href="#L176">176</a>
<a href="#L177">177</a>
<a href="#L178">178</a>
<a href="#L179">179</a>
<a href="#L180">180</a>
<a href="#L181">181</a>
<a href="#L182">182</a>
<a href="#L183">183</a>
<a href="#L184">184</a>
<a href="#L185">185</a>
<a href="#L186">186</a>
<a href="#L187">187</a>
<a href="#L188">188</a>
<a href="#L189">189</a>
<a href="#L190">190</a>
<a href="#L191">191</a>
<a href="#L192">192</a>
<a href="#L193">193</a>
<a href="#L194">194</a>
<a href="#L195">195</a>
<a href="#L196">196</a>
<a href="#L197">197</a>
<a href="#L198">198</a>
<a href="#L199">199</a>
<a href="#L200">200</a>
<a href="#L201">201</a>
<a href="#L202">202</a>
<a href="#L203">203</a>
<a href="#L204">204</a>
<a href="#L205">205</a>
<a href="#L206">206</a>
<a href="#L207">207</a>
<a href="#L208">208</a>
<a href="#L209">209</a>
<a href="#L210">210</a>
<a href="#L211">211</a>
<a href="#L212">212</a>
<a href="#L213">213</a>
<a href="#L214">214</a>
<a href="#L215">215</a>
<a href="#L216">216</a>
<a href="#L217">217</a>
<a href="#L218">218</a>
<a href="#L219">219</a>
<a href="#L220">220</a>
<a href="#L221">221</a>
<a href="#L222">222</a>
<a href="#L223">223</a>
<a href="#L224">224</a>
<a href="#L225">225</a>
<a href="#L226">226</a>
<a href="#L227">227</a>
<a href="#L228">228</a>
<a href="#L229">229</a>
<a href="#L230">230</a>
<a href="#L231">231</a>
<a href="#L232">232</a>
<a href="#L233">233</a>
<a href="#L234">234</a>
<a href="#L235">235</a>
<a href="#L236">236</a>
<a href="#L237">237</a>
<a href="#L238">238</a>
<a href="#L239">239</a>
<a href="#L240">240</a>
<a href="#L241">241</a>
<a href="#L242">242</a>
<a href="#L243">243</a>
<a href="#L244">244</a>
<a href="#L245">245</a>
<a href="#L246">246</a>
<a href="#L247">247</a>
<a href="#L248">248</a>
<a href="#L249">249</a>
<a href="#L250">250</a>
<a href="#L251">251</a>
<a href="#L252">252</a>
<a href="#L253">253</a>
<a href="#L254">254</a>
<a href="#L255">255</a>
<a href="#L256">256</a>
<a href="#L257">257</a>
<a href="#L258">258</a>
<a href="#L259">259</a>
<a href="#L260">260</a>
<a href="#L261">261</a>
<a href="#L262">262</a>
<a href="#L263">263</a>
<a href="#L264">264</a>
<a href="#L265">265</a>
<a href="#L266">266</a>
<a href="#L267">267</a>
<a href="#L268">268</a>
<a href="#L269">269</a>
<a href="#L270">270</a>
<a href="#L271">271</a>
<a href="#L272">272</a>
<a href="#L273">273</a>
<a href="#L274">274</a>
<a href="#L275">275</a>
<a href="#L276">276</a>
<a href="#L277">277</a>
<a href="#L278">278</a>
<a href="#L279">279</a>
<a href="#L280">280</a>
<a href="#L281">281</a>
<a href="#L282">282</a>
<a href="#L283">283</a>
<a href="#L284">284</a>
<a href="#L285">285</a>
<a href="#L286">286</a>
<a href="#L287">287</a>
<a href="#L288">288</a>
<a href="#L289">289</a>
<a href="#L290">290</a>
<a href="#L291">291</a>
<a href="#L292">292</a>
<a href="#L293">293</a>
<a href="#L294">294</a>
<a href="#L295">295</a>
<a href="#L296">296</a>
<a href="#L297">297</a>
<a href="#L298">298</a>
<a href="#L299">299</a>
<a href="#L300">300</a>
<a href="#L301">301</a>
<a href="#L302">302</a>
<a href="#L303">303</a>
<a href="#L304">304</a>
<a href="#L305">305</a>
<a href="#L306">306</a>
<a href="#L307">307</a>
<a href="#L308">308</a>
<a href="#L309">309</a>
<a href="#L310">310</a>
<a href="#L311">311</a>
<a href="#L312">312</a>
<a href="#L313">313</a>
<a href="#L314">314</a>
<a href="#L315">315</a>
<a href="#L316">316</a>
<a href="#L317">317</a>
<a href="#L318">318</a>
<a href="#L319">319</a>
<a href="#L320">320</a>
<a href="#L321">321</a>
<a href="#L322">322</a>
<a href="#L323">323</a>
<a href="#L324">324</a>
<a href="#L325">325</a>
<a href="#L326">326</a>
<a href="#L327">327</a>
<a href="#L328">328</a>
<a href="#L329">329</a>
<a href="#L330">330</a>
<a href="#L331">331</a>
<a href="#L332">332</a>
<a href="#L333">333</a>
<a href="#L334">334</a>
<a href="#L335">335</a>
<a href="#L336">336</a>
<a href="#L337">337</a>
<a href="#L338">338</a>
<a href="#L339">339</a>
<a href="#L340">340</a>
<a href="#L341">341</a>
<a href="#L342">342</a>
<a href="#L343">343</a>
<a href="#L344">344</a>
<a href="#L345">345</a>
<a href="#L346">346</a>
<a href="#L347">347</a>
<a href="#L348">348</a>
<a href="#L349">349</a>
<a href="#L350">350</a>
<a href="#L351">351</a>
<a href="#L352">352</a>
<a href="#L353">353</a>
<a href="#L354">354</a>
<a href="#L355">355</a>
<a href="#L356">356</a>
<a href="#L357">357</a>
<a href="#L358">358</a>
<a href="#L359">359</a>
<a href="#L360">360</a>
<a href="#L361">361</a>
<a href="#L362">362</a>
<a href="#L363">363</a>
<a href="#L364">364</a>
<a href="#L365">365</a>
<a href="#L366">366</a>
<a href="#L367">367</a>
<a href="#L368">368</a>
<a href="#L369">369</a>
<a href="#L370">370</a>
<a href="#L371">371</a>
<a href="#L372">372</a>
<a href="#L373">373</a>
<a href="#L374">374</a>
<a href="#L375">375</a>
<a href="#L376">376</a>
<a href="#L377">377</a>
<a href="#L378">378</a>
<a href="#L379">379</a>
<a href="#L380">380</a>
<a href="#L381">381</a>
<a href="#L382">382</a>
<a href="#L383">383</a>
<a href="#L384">384</a>
<a href="#L385">385</a>
<a href="#L386">386</a>
<a href="#L387">387</a>
<a href="#L388">388</a>
<a href="#L389">389</a>
<a href="#L390">390</a>
<a href="#L391">391</a>
<a href="#L392">392</a>
<a href="#L393">393</a>
<a href="#L394">394</a>
<a href="#L395">395</a>
<a href="#L396">396</a>
<a href="#L397">397</a>
<a href="#L398">398</a>
<a href="#L399">399</a>
<a href="#L400">400</a>
<a href="#L401">401</a>
<a href="#L402">402</a>
<a href="#L403">403</a>
<a href="#L404">404</a>
<a href="#L405">405</a>
<a href="#L406">406</a>
<a href="#L407">407</a>
<a href="#L408">408</a>
<a href="#L409">409</a>
<a href="#L410">410</a>
<a href="#L411">411</a>
<a href="#L412">412</a>
<a href="#L413">413</a>
<a href="#L414">414</a>
<a href="#L415">415</a>
<a href="#L416">416</a>
<a href="#L417">417</a>
<a href="#L418">418</a>
<a href="#L419">419</a>
<a href="#L420">420</a>
<a href="#L421">421</a>
<a href="#L422">422</a>
<a href="#L423">423</a>
<a href="#L424">424</a>
<a href="#L425">425</a>
<a href="#L426">426</a>
<a href="#L427">427</a>
<a href="#L428">428</a>
<a href="#L429">429</a>
<a href="#L430">430</a>
<a href="#L431">431</a>
<a href="#L432">432</a>
<a href="#L433">433</a>
<a href="#L434">434</a>
<a href="#L435">435</a>
<a href="#L436">436</a>
<a href="#L437">437</a>
<a href="#L438">438</a>
<a href="#L439">439</a>
<a href="#L440">440</a>
<a href="#L441">441</a>
<a href="#L442">442</a>
<a href="#L443">443</a>
<a href="#L444">444</a>
<a href="#L445">445</a>
<a href="#L446">446</a>
<a href="#L447">447</a>
<a href="#L448">448</a>
<a href="#L449">449</a>
<a href="#L450">450</a>
<a href="#L451">451</a>
<a href="#L452">452</a>
<a href="#L453">453</a>
<a href="#L454">454</a>
<a href="#L455">455</a>
<a href="#L456">456</a>
<a href="#L457">457</a>
<a href="#L458">458</a>
<a href="#L459">459</a>
<a href="#L460">460</a>
<a href="#L461">461</a>
<a href="#L462">462</a>
<a href="#L463">463</a>
<a href="#L464">464</a>
<a href="#L465">465</a>
<a href="#L466">466</a>
<a href="#L467">467</a>
<a href="#L468">468</a>
<a href="#L469">469</a>
<a href="#L470">470</a>
<a href="#L471">471</a>
<a href="#L472">472</a>
<a href="#L473">473</a>
<a href="#L474">474</a>
<a href="#L475">475</a>
<a href="#L476">476</a>
<a href="#L477">477</a>
<a href="#L478">478</a>
<a href="#L479">479</a>
<a href="#L480">480</a>
<a href="#L481">481</a>
<a href="#L482">482</a>
<a href="#L483">483</a>
<a href="#L484">484</a>
<a href="#L485">485</a>
<a href="#L486">486</a>
<a href="#L487">487</a>
<a href="#L488">488</a>
<a href="#L489">489</a>
<a href="#L490">490</a>
<a href="#L491">491</a>
<a href="#L492">492</a>
<a href="#L493">493</a>
<a href="#L494">494</a>
<a href="#L495">495</a>
<a href="#L496">496</a>
<a href="#L497">497</a>
<a href="#L498">498</a>
<a href="#L499">499</a>
<a href="#L500">500</a>
<a href="#L501">501</a>
<a href="#L502">502</a>
<a href="#L503">503</a>
<a href="#L504">504</a>
<a href="#L505">505</a>
<a href="#L506">506</a>
<a href="#L507">507</a>
<a href="#L508">508</a>
<a href="#L509">509</a>
<a href="#L510">510</a>
<a href="#L511">511</a>
<a href="#L512">512</a>
<a href="#L513">513</a>
<a href="#L514">514</a>
<a href="#L515">515</a>
<a href="#L516">516</a>
<a href="#L517">517</a>
<a href="#L518">518</a>
<a href="#L519">519</a>
<a href="#L520">520</a>
<a href="#L521">521</a>
<a href="#L522">522</a>
<a href="#L523">523</a>
<a href="#L524">524</a>
<a href="#L525">525</a>
<a href="#L526">526</a>
<a href="#L527">527</a>
<a href="#L528">528</a>
<a href="#L529">529</a>
<a href="#L530">530</a>
<a href="#L531">531</a>
<a href="#L532">532</a>
<a href="#L533">533</a>
<a href="#L534">534</a>
<a href="#L535">535</a>
<a href="#L536">536</a>
<a href="#L537">537</a>
<a href="#L538">538</a>
<a href="#L539">539</a>
<a href="#L540">540</a>
<a href="#L541">541</a>
<a href="#L542">542</a>
<a href="#L543">543</a>
<a href="#L544">544</a>
<a href="#L545">545</a>
<a href="#L546">546</a>
</pre>
<pre><code class="ocaml">open OUnit2
open Core

module T = Tensor
module N = Ndarray

let float_array_equal ~(epsilon:float) arr1 arr2 =
  <span data-count="30">l</span>et len1 = Array.length arr1 in
  <span data-count="30">l</span>et len2 = Array.length arr2 in
  <span data-count="30">i</span>f len1 &lt;&gt; len2 then <span data-count="0">f</span>alse else
    <span data-count="30">A</span>rray.for_alli arr1 ~f:(fun i x -&gt;
      <span data-count="141">F</span>loat.(ab<span data-count="141">s</span> (x -. arr2.(i)) &lt; epsilon))

let float_ndarray_equal ~epsilon nd1 nd2 =
  <span data-count="30">l</span>et arr1 = N.to_array nd1 in
  <span data-count="30">l</span>et arr2 = N.to_array nd2 in
  <span data-count="30">f</span>loat_array_equal ~epsilon arr1 arr2

let shape_equal s1 s2 =
  <span data-count="8">l</span>et len1 = Array.length s1 in
  <span data-count="8">l</span>et len2 = Array.length s2 in
  <span data-count="8">l</span>en1 = len2 &amp;&amp; <span data-count="8">A</span>rray.for_alli s1 ~f:(fun i x -&gt; <span data-count="16">x</span> = s2.(i))
  
  
let eps = 1e-7

(* Basic tests are assumed to be defined in a similar manner as before. *)
(* Below we add tests for broadcasting scenarios. *)

let test_broadcast_add _ =
  (* Shape [2;3] + [3] should broadcast to [2;3].
     For example: [ [1,2,3],
                    [4,5,6] ]
     plus [10,20,30]
     = [ [11,22,33],
         [14,25,36] ]
  *)
  <span data-count="1">l</span>et t1 = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.;5.;6.|] [|2;3|]) in
  <span data-count="1">l</span>et t2 = T.from_ndarray (N.creat<span data-count="1">e</span> [|10.;20.;30.|] [|3|]) in
  <span data-count="1">l</span>et out = T.add t1 t2 in
  <span data-count="1">l</span>et expected = N.create [|11.;22.;33.;14.;25.;36.|] [|2;3|] in
  <span data-count="1">a</span>ssert_bool "Broadcast add forward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t1; <span data-count="1">T</span>.zero_grad t2; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;3|]; (* gradient is all ones *)
  (match out.T.backward_fn with
   | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> ()
   | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for add");

  (* Gradient checks:
     For t1 (same shape), gradient is just the out gradient: all ones in shape [2;3].
     For t2 (shape [3]), the gradient must be reduced (summing over the first dimension):
     sum over the 2 rows: each element in t2’s grad = sum of two gradients from each row
     So t2 grad = [2;2;2]
  *)
  let expected_grad_t1 = N.ones [|2;3|] in
  <span data-count="1">l</span>et expected_grad_t2 = N.create [|2.;2.;2.|] [|3|] in
  <span data-count="1">a</span>ssert_bool "Grad t1 broadcast add" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t1.T.grad expected_grad_t1);
  <span data-count="1">a</span>ssert_bool "Grad t2 broadcast add" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t2.T.grad expected_grad_t2)


let test_broadcast_mul _ =
  (* Shape [2;3] * [3] should broadcast:
     t1 = [[1,2,3],[4,5,6]]
     t2 = [10,20,30]

     out = [[1*10,2*20,3*30],
            [4*10,5*20,6*30]]
         = [[10,40,90],
            [40,100,180]]
  *)
  <span data-count="1">l</span>et t1 = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.;5.;6.|] [|2;3|]) in
  <span data-count="1">l</span>et t2 = T.from_ndarray (N.creat<span data-count="1">e</span> [|10.;20.;30.|] [|3|]) in
  <span data-count="1">l</span>et out = T.mul t1 t2 in
  <span data-count="1">l</span>et expected = N.create [|10.;40.;90.;40.;100.;180.|] [|2;3|] in
  <span data-count="1">a</span>ssert_bool "Broadcast mul forward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t1; <span data-count="1">T</span>.zero_grad t2; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;3|]; 
  (match out.T.backward_fn with
   | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> ()
   | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for mul");

  (* Grad checks:
     dOut/dt1 = t2 broadcasted = [[10,20,30],[10,20,30]]
     So t1.grad = sum over out gradient * t2:
        all ones * t2 = [[10,20,30],[10,20,30]]
     t1.grad shape [2;3], equals [[10,20,30],[10,20,30]]

     dOut/dt2 = t1 broadcasted, but must be reduced to shape [3]:
        sum over dimension 0:
        Column 0: 1 + 4 = 5
        Column 1: 2 + 5 = 7
        Column 2: 3 + 6 = 9
     t2.grad = [5,7,9]
  *)
  let expected_grad_t1 = N.create [|10.;20.;30.;10.;20.;30.|] [|2;3|] in
  <span data-count="1">l</span>et expected_grad_t2 = N.create [|5.;7.;9.|] [|3|] in

  <span data-count="1">a</span>ssert_bool "Grad t1 broadcast mul"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t1.T.grad expected_grad_t1);
  <span data-count="1">a</span>ssert_bool "Grad t2 broadcast mul"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t2.T.grad expected_grad_t2)


let test_broadcast_sub _ =
  (* Shape [2;3] - [3]:
     t1 = [[10,11,12],[13,14,15]]
     t2 = [1,2,3]
     out = [[9,9,9],[12,12,12]]

  *)
  <span data-count="1">l</span>et t1 = T.from_ndarray (N.creat<span data-count="1">e</span> [|10.;11.;12.;13.;14.;15.|] [|2;3|]) in
  <span data-count="1">l</span>et t2 = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.|] [|3|]) in
  <span data-count="1">l</span>et out = T.sub t1 t2 in
  <span data-count="1">l</span>et expected = N.create [|9.;9.;9.;12.;12.;12.|] [|2;3|] in
  <span data-count="1">a</span>ssert_bool "Broadcast sub forward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t1; <span data-count="1">T</span>.zero_grad t2; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;3|]; 
  (match out.T.backward_fn with
   | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> ()
   | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for sub");

  (* Grad checks:
     For t1: Grad is all ones, same shape [2;3].
     For t2: Grad is negative ones summed over dim 0:
       Each element in t2.grad = sum of -1 from each row:
       t2.grad = [-2, -2, -2]
  *)
  let expected_grad_t1 = N.ones [|2;3|] in
  <span data-count="1">l</span>et expected_grad_t2 = N.create [|-2.;-2.;-2.|] [|3|] in
  <span data-count="1">a</span>ssert_bool "Grad t1 broadcast sub"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t1.T.grad expected_grad_t1);
  <span data-count="1">a</span>ssert_bool "Grad t2 broadcast sub"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t2.T.grad expected_grad_t2)


(* If division also supports broadcasting, test similarly *)
let test_broadcast_div _ =
  (* Shape [2;3] / [3]:
     t1 = [[2,4,6],[8,10,12]]
     t2 = [1,2,3]
     out = [[2/1,4/2,6/3],[8/1,10/2,12/3]]
         = [[2,2,2],[8,5,4]]
  *)
  <span data-count="1">l</span>et t1 = T.from_ndarray (N.creat<span data-count="1">e</span> [|2.;4.;6.;8.;10.;12.|] [|2;3|]) in
  <span data-count="1">l</span>et t2 = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.|] [|3|]) in
  <span data-count="1">l</span>et out = T.div t1 t2 in
  <span data-count="1">l</span>et expected = N.create [|2.;2.;2.;8.;5.;4.|] [|2;3|] in
  <span data-count="1">a</span>ssert_bool "Broadcast div forward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t1; <span data-count="1">T</span>.zero_grad t2; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;3|]; 
  (match out.T.backward_fn with
   | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () 
   | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for div");

  (* Grad checks:
     dOut/dt1 = 1/t2 broadcasted:
       = [[1/1,1/2,1/3],[1/1,1/2,1/3]]
       = [[1,0.5,0.3333],[1,0.5,0.3333]]

     sum over entire t1 since same shape, no reduction needed.

     dOut/dt2 = - (t1 / t2^2):
       For each element:
       row 0: [-2/1^2, -4/2^2, -6/3^2] = [-2, -1, -6/9 = -0.6667]
       row 1: [-8/1, -10/4, -12/9] = [-8, -2.5, -1.3333]
       
     Sum over the 2 rows for t2’s grad:
       t2[0]: -2 + (-8) = -10
       t2[1]: -1 + (-2.5) = -3.5
       t2[2]: -0.6667 + (-1.3333) = -2.0 (approx)
  *)
  let expected_grad_t1 = N.create [|1.;0.5;1./.3.;1.;0.5;1./.3.|] [|2;3|] in
  <span data-count="1">l</span>et expected_grad_t2 = N.create [|-10.;-3.5; -2.|] [|3|] in
  <span data-count="1">a</span>ssert_bool "Grad t1 broadcast div"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:1e-5 t1.T.grad expected_grad_t1);
  <span data-count="1">a</span>ssert_bool "Grad t2 broadcast div"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:1e-5 t2.T.grad expected_grad_t2)

    
  (* Test creation functions *)
  let test_create_basic _ =
    <span data-count="1">l</span>et data = N.ones [|2;2|] in
    <span data-count="1">l</span>et t = T.create ~data ~requires_grad:true ~prev:[] in
    assert_bool "check data shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) [|2;2|]);
    <span data-count="1">a</span>ssert_bool "check grad shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.grad) [|2;2|]);
    <span data-count="1">a</span>ssert_equal true (T.requires_gra<span data-count="1">d</span> t)
  
  let test_from_ndarray_requires_grad_false _ =
    <span data-count="1">l</span>et data = N.arange 6. in
    <span data-count="1">l</span>et t = T.from_ndarray ~requires_grad:false data in
    <span data-count="1">a</span>ssert_equal false (T.requires_gra<span data-count="1">d</span> t)
  
  let test_zeros_shape _ =
    <span data-count="1">l</span>et t = T.zeros [|3;4|] in
    <span data-count="1">a</span>ssert_bool "zeros shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) [|3;4|]);
    <span data-count="1">a</span>ssert_bool "zeros values" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.data (N.zero<span data-count="1">s</span> [|3;4|]))
  
  let test_ones_shape _ =
    <span data-count="1">l</span>et t = T.ones [|2;3|] in
    <span data-count="1">a</span>ssert_bool "ones shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) [|2;3|]);
    <span data-count="1">a</span>ssert_bool "ones values" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.data (N.one<span data-count="1">s</span> [|2;3|]))
  
  let test_rand_shape _ =
    <span data-count="1">l</span>et shape = [|2;2|] in
    let t = T.rand shape in
    <span data-count="1">a</span>ssert_bool "rand shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) shape);
    (* Can't predict values, but can check range if Ndarray.rand is uniform in [0,1) *)
    <span data-count="1">l</span>et arr = N.to_array t.T.data in
    <span data-count="1">a</span>ssert_bool "all values in [0,1)" Float.(Array.for_al<span data-count="1">l</span> arr ~f:(fun x -&gt; <span data-count="4">x</span> &gt;= 0.0 &amp;&amp; <span data-count="4">x</span> &lt; 1.0))
  
  let test_xavier_init_shape _ =
    <span data-count="1">l</span>et shape = [|3;3|] in
    let t = T.xavier_init shape in
    <span data-count="1">a</span>ssert_bool "xavier shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) shape)
  
  let test_he_init_shape _ =
    <span data-count="1">l</span>et shape = [|4;5|] in
    let t = T.he_init shape in
    <span data-count="1">a</span>ssert_bool "he_init shape" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> t.T.data) shape)
  
  (* Test get and set *)
  let test_get_set_values _ =
    <span data-count="1">l</span>et t = T.ones [|2;2|] in
    <span data-count="1">T</span>.set t [|0;1|] 5.0;
    <span data-count="1">a</span>ssert_equal 5.0 (T.ge<span data-count="1">t</span> t [|0;1|]);
    <span data-count="1">T</span>.set t [|1;0|] (-3.0);
    <span data-count="1">a</span>ssert_equal (-3.0) (T.ge<span data-count="1">t</span> t [|1;0|])
  
  (* Test arithmetic operations *)
  let test_add_grad_accumulate _ =
    <span data-count="1">l</span>et t1 = T.ones [|2;2|] in
    <span data-count="1">l</span>et t1 = T.transpose t1 in 
    <span data-count="1">l</span>et t2 = T.ones [|2;2|] in
    <span data-count="1">l</span>et out = T.add t1 t2 in
    <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;2|]; 
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* Check gradient accumulation by calling backward again *)
    out.T.grad &lt;- N.one<span data-count="1">s</span> [|2;2|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* Now t1 and t2 grads should be 2 * ones *)
    let expected =N.zeros [|2;2|] in
    <span data-count="1">N</span>.fill expected 2.0;
    <span data-count="1">a</span>ssert_bool "grad accumulation t1" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t1.T.grad expected);
    <span data-count="1">a</span>ssert_bool "grad accumulation t2" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t2.T.grad expected)
  
  (* let test_sub_nonrequiring_grad _ =
    let t1 = T.from_ndarray ~requires_grad:false (N.ones [|2;2|]) in
    let t2 = T.ones [|2;2|] in
    let out = T.sub t1 t2 in
    out.T.grad &lt;- N.ones [|2;2|];
    (match out.T.backward_fn with Some fn -&gt; fn () | None -&gt; ());
    (* t1 does not require grad, so it stays zero *)
    let expected_t1_grad = N.zeros [|2;2|] in
    let expected_t2_grad = N.full [|2;2|] (-1.0) in
    assert_bool "t1 no grad" (float_ndarray_equal ~epsilon:eps t1.T.grad expected_t1_grad);
    assert_bool "t2 grad" (float_ndarray_equal ~epsilon:eps t2.T.grad expected_t2_grad)
   *)
  
  (* Test matmul with higher dimensions if supported, or just standard *)
  let test_matmul_non_square _ =
    <span data-count="1">l</span>et t1 = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.;5.;6.|] [|2;3|]) in
    <span data-count="1">l</span>et t2 = T.from_ndarray (N.creat<span data-count="1">e</span> [|7.;8.;9.;10.;11.;12.|] [|3;2|]) in
    (* out shape = [2;2] *)
    <span data-count="1">l</span>et out = T.matmul t1 t2 in
    (* Manual matmul:
        [ [1*7+2*9+3*11, 1*8+2*10+3*12],
          [4*7+5*9+6*11, 4*8+5*10+6*12] ]
        Wait carefully:
        It's (2x3) * (3x2):
          out[0,0] = 1*7 + 2*9 + 3*11 = 7 +18 +33 =58
          out[0,1] = 1*8 + 2*10 + 3*12 =8+20+36=64
          out[1,0] = 4*7 +5*9 +6*11 =28+45+66=139
          out[1,1] = 4*8 +5*10+6*12=32+50+72=154
      *)
    <span data-count="1">l</span>et expected = N.create [|58.;64.;139.;154.|] [|2;2|] in
    <span data-count="1">a</span>ssert_bool "matmul forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;2|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* Check gradients roughly:
        d(t1) = dOut * t2^T
        t2^T = [[7,9,11],[8,10,12]]
        dOut = ones:
          d(t1) shape = [2;3]:
          row0: sum along row of t2^T: (7+8)=15, (9+10)=19, (11+12)=23
          Actually, we must do full matmul:
          d(t1) = [[1,1];[1,1]] * t2^T = 
            For first row of t1:
              [ (1*7+1*8), (1*9+1*10), (1*11+1*12) ] = [15,19,23]
            For second row of t1:
              the same since gradient is all ones: [15,19,23]
  
        so t1.grad = [[15,19,23],[15,19,23]]
  
        d(t2) = t1^T * dOut
        t1^T = [[1,4],[2,5],[3,6]]
        dOut = ones:
          Each element of t2:
            [ (1+4),(2+5),(3+6) ; same for second column ]
          Actually:
          d(t2) shape [3;2]:
            For column0: sum first column of t1^T with out grads:
            t2 grad col0: 
              [ (1*1+4*1) , (2*1+5*1), (3*1+6*1)] = [5,7,9]
          For column1: same:
              [5,7,9]
  
        t2.grad = [[5,5],[7,7],[9,9]] after proper dimension checks. Wait carefully:
        d(t2) = (t1^T (2x3)) * (dOut (2x2)):
          t1^T is (3x2), dOut is (2x2), so result (3x2)
          d(t2)[0,0] = (1*1 +4*1)=5; d(t2)[0,1]=(1*1+4*1)=5
          d(t2)[1,0]=(2*1+5*1)=7; d(t2)[1,1]=7
          d(t2)[2,0]=(3*1+6*1)=9; d(t2)[2,1]=9
  
    *)
    let expected_t1_grad = N.create [|15.;19.;23.;15.;19.;23.|] [|2;3|] in
    <span data-count="1">l</span>et expected_t2_grad = N.create [|5.;5.;7.;7.;9.;9.|] [|3;2|] in
    <span data-count="1">a</span>ssert_bool "t1 grad matmul" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t1.T.grad expected_t1_grad);
    <span data-count="1">a</span>ssert_bool "t2 grad matmul" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t2.T.grad expected_t2_grad)
  
  (* Test transpose *)
  (* let test_transpose_backward_multiple _ =
    let t = T.arange 6. in
    let t = T.reshape t ~shape:[|2;3|] in
    let out = T.transpose t in
    out.T.grad &lt;- N.ones [|3;2|];
    (match out.T.backward_fn with Some fn -&gt; fn () | None -&gt; ());
    (* transpose again to verify correct accumulation *)
    out.T.grad &lt;- N.ones [|3;2|];
    (match out.T.backward_fn with Some fn -&gt; fn () | None -&gt; ());
    (* Each backward adds ones transpose again, 
        grad_t after two backward calls should be [|2;3|] of all 2's *)
    let expected = N.full [|2;3|] 2. in
    assert_bool "transpose grad accumulation" (float_ndarray_equal ~epsilon:eps t.T.grad expected)
   *)

  (* Test reshape *)
  let test_reshape_mismatch _ =
    <span data-count="1">l</span>et t = T.arange 8. in
    <span data-count="1">l</span>et out = T.reshape t ~shape:[|2;4|] in
    <span data-count="1">a</span>ssert_bool "reshape forward" (shape_equa<span data-count="1">l</span> (N.shap<span data-count="1">e</span> out.T.data) [|2;4|]);
    <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2;4|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* Grad should reshape back to original [|8|] *)
    let expected = N.ones [|8|] in
    <span data-count="1">a</span>ssert_bool "reshape backward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected)
  
  (* Test sum over different dims *)
  let test_sum_no_requires_grad _ =
    <span data-count="1">l</span>et t = T.from_ndarray ~requires_grad:false (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.|] [|2;2|]) in
    <span data-count="1">l</span>et out = T.sum ~dim:1 t in
    (* sum along dim 1: [[1+2],[3+4]] = [3,7] *)
    <span data-count="1">l</span>et expected = N.create [|3.;7.|] [|2|] in
    <span data-count="1">a</span>ssert_bool "sum forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|2|];
    (match out.T.backward_fn with <span data-count="0">S</span>ome fn -&gt; f<span data-count="0">n</span> () | <span data-count="1">N</span>one -&gt; ());
    (* t doesn't require grad, so no grad accumulation *)
    let expected_grad_t = N.zeros [|2;2|] in
    <span data-count="1">a</span>ssert_bool "no grad for sum input" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad_t)
  
  (* Test sum multiple dims *)
  let test_sum_multiple_dims _ =
    <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.;5.;6.|] [|2;3|]) in
    (* sum along dim=0 *)
    <span data-count="1">l</span>et out = T.sum ~dim:0 t in
    (* sum along dim 0: [[1+4],[2+5],[3+6]] = [5,7,9] -&gt; shape [3] *)
    <span data-count="1">l</span>et expected = N.create [|5.;7.;9.|] [|3|] in
    <span data-count="1">a</span>ssert_bool "sum along dim 0" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|3|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* This should broadcast back along dim 0:
        Original shape: [2;3]
        After sum dim 0, we got shape [3]
        Gradient should expand to two rows:
        [[1,1,1],
        [1,1,1]]
    *)
    let expected_grad = N.ones [|2;3|] in
    <span data-count="1">a</span>ssert_bool "sum backward dim 0" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)
  
  (* Test mean with different dims *)
  let test_mean_dim _ =
    <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.;2.;3.;4.;5.;6.|] [|2;3|]) in
    <span data-count="1">l</span>et out = T.mean ~dim:1 t in
    (* mean dim 1: row means -&gt; [ (1+2+3)/3, (4+5+6)/3 ] = [2,5] *)
    <span data-count="1">l</span>et expected = N.create [|2.;5.|] [|2|] in
    <span data-count="1">a</span>ssert_bool "mean forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.creat<span data-count="1">e</span> [|2.;1.|] [|2|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* Backward:
        grad_input = grad_output / 3 broadcasted:
        [[2/3,2/3,2/3],[1/3,1/3,1/3]]
    *)
    let expected_grad = N.create [|(2./.3.);(2./.3.);(2./.3.);(1./.3.);(1./.3.);(1./.3.)|] [|2;3|] in
    <span data-count="1">a</span>ssert_bool "mean backward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)
  
  (* Test neg *)
  let test_neg_no_requires_grad _ =
    <span data-count="1">l</span>et t = T.from_ndarray ~requires_grad:false (N.creat<span data-count="1">e</span> [|1.;-2.|] [|2|]) in
    <span data-count="1">l</span>et out = T.neg t in
    <span data-count="1">l</span>et expected = N.create [|-1.;2.|] [|2|] in
    <span data-count="1">a</span>ssert_bool "neg forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.creat<span data-count="1">e</span> [|1.;1.|] [|2|];
    (match out.T.backward_fn with <span data-count="0">S</span>ome fn -&gt; f<span data-count="0">n</span> () | <span data-count="1">N</span>one -&gt; ());
    (* t does not require grad, so no grad update *)
    let expected_grad = N.zeros [|2|] in
    <span data-count="1">a</span>ssert_bool "no grad for neg input" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)
  
  (* Test relu with all negative *)
  let test_relu_all_negative _ =
    <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|-5.; -4.|] [|2|]) in
    <span data-count="1">l</span>et out = T.relu t in
    <span data-count="1">l</span>et expected = N.zeros [|2|] in
    <span data-count="1">a</span>ssert_bool "relu forward negative"
      (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);
    <span data-count="1">o</span>ut.T.grad &lt;- N.creat<span data-count="1">e</span> [|10.;10.|] [|2|];
    (match out.T.backward_fn with <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> () | <span data-count="0">N</span>one -&gt; ());
    (* All negatives result in zero gradient back *)
    let expected_grad = N.zeros [|2|] in
    <span data-count="1">a</span>ssert_bool "relu backward all negative"
      (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)


let float_array_equal ~(epsilon:float) arr1 arr2 =
  <span data-count="5">l</span>et len1 = Array.length arr1 in
  <span data-count="5">l</span>et len2 = Array.length arr2 in
  <span data-count="5">i</span>f len1 &lt;&gt; len2 then <span data-count="0">f</span>alse else
    <span data-count="5">A</span>rray.for_alli arr1 ~f:(fun i x -&gt;
      <span data-count="18">F</span>loat.(ab<span data-count="18">s</span> (x -. arr2.(i)) &lt; epsilon))

let float_ndarray_equal ~epsilon nd1 nd2 =
  <span data-count="5">l</span>et arr1 = N.to_array nd1 in
  <span data-count="5">l</span>et arr2 = N.to_array nd2 in
  <span data-count="5">f</span>loat_array_equal ~epsilon arr1 arr2


let eps = 1e-7


(* Test softmax *)
let test_softmax _ =
  <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.; 2.; 3.; 4.|] [|4|]) ~requires_grad:true in
  <span data-count="1">l</span>et out = T.softmax t in
  <span data-count="1">l</span>et max_val = 4. in
  let shifted_logits = [|1. -. max_val; 2. -. max_val; 3. -. max_val; 4. -. max_val|] in
  let exp_logits = Stdlib.Array.map exp shifted_logits in
  <span data-count="1">l</span>et sum_exp = Array.fold ~init:0. ~f:(+.) exp_logits in
  <span data-count="1">l</span>et expected = Stdlib.Array.map (fun x -&gt; <span data-count="4">x</span> /. sum_exp) exp_logits in
  <span data-count="1">l</span>et expected = N.create expected [|4|] in
  <span data-count="1">a</span>ssert_bool "Softmax forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|4|];
  (match out.T.backward_fn with
    | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> ()
    | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for softmax");

  (* Check gradient *)
  let softmax_vals = Stdlib.Array.map (fun x -&gt; <span data-count="4">x</span> /. sum_exp) exp_logits in
  <span data-count="1">l</span>et softmax_nd = N.create softmax_vals [|4|] in
  <span data-count="1">l</span>et sum_grad = N.dsum (N.mu<span data-count="1">l</span> out.T.grad softmax_nd) 0 in
  <span data-count="1">l</span>et expected_grad = N.mul softmax_nd (N.su<span data-count="1">b</span> out.T.grad sum_grad) in
  <span data-count="1">a</span>ssert_bool "Softmax backward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)

(* Test log_softmax *)
let test_log_softmax _ =

  <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.; 2.; 3.; 4.|] [|4|]) ~requires_grad:true in
  <span data-count="1">l</span>et out = T.log_softmax t in
  <span data-count="1">l</span>et max_val = 4. in
  let shifted_logits = [|1. -. max_val; 2. -. max_val; 3. -. max_val; 4. -. max_val|] in
  let sum_exp = Array.fold ~init:0. ~f:(+.) (Stdlib.Array.ma<span data-count="1">p</span> Float.exp shifted_logits) in
  <span data-count="1">l</span>et log_probs = Stdlib.Array.map (fun x -&gt; <span data-count="4">x</span> -. Float.lo<span data-count="4">g</span> sum_exp) shifted_logits in 
  <span data-count="1">l</span>et log_probs = N.create log_probs [|4|] in
  <span data-count="1">a</span>ssert_bool "Log-Softmax forward" (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data log_probs);

  (* Backward *)
  <span data-count="1">T</span>.zero_grad t; <span data-count="1">T</span>.zero_grad out;
  <span data-count="1">o</span>ut.T.grad &lt;- N.one<span data-count="1">s</span> [|4|];
  (match out.T.backward_fn with
    | <span data-count="1">S</span>ome fn -&gt; f<span data-count="1">n</span> ()
    | <span data-count="0">N</span>one -&gt; assert_failur<span data-count="0">e</span> "No backward for log_softmax");

  (* Check gradient *)
  let softmax_vals = Stdlib.Array.map (fun x -&gt; <span data-count="4">F</span>loat.ex<span data-count="4">p</span> x /. sum_exp) shifted_logits in
  <span data-count="1">l</span>et softmax_nd = N.create softmax_vals [|4|] in
  <span data-count="1">l</span>et sum_grad = N.dsum (N.mu<span data-count="1">l</span> out.T.grad softmax_nd) 0 in
  <span data-count="1">l</span>et expected_grad = N.sub out.T.grad (N.mu<span data-count="1">l</span> softmax_nd sum_grad) in
  <span data-count="1">a</span>ssert_bool "Log-Softmax backward"
    (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps t.T.grad expected_grad)

  let test_slice _ = 
    <span data-count="1">l</span>et t = T.from_ndarray (N.creat<span data-count="1">e</span> [|1.; 2.; 3.; 4.; 5.; 6.; 7.; 8.|] [|2; 4|]) ~requires_grad:true in
    
    (* Correct the slice range: The original range [(0, 1); (1, 3)] is valid, 
        but the actual test might not handle the indices correctly. *)
    <span data-count="1">l</span>et out = T.slice t [(0, 1); (1, 3)] in
    (* Correct the expected output: Extract elements [2; 3] within the range, with shape [1; 2]. *)
    <span data-count="1">l</span>et expected_data = N.create [|2.; 3.|] [|1; 2|] in
  
    (* Verify whether the forward pass output is correct *)
    <span data-count="1">a</span>ssert_bool "Slice forward"
      (float_ndarray_equa<span data-count="1">l</span> ~epsilon:eps out.T.data expected_data)

let suite =
  "Test Tensor Broadcast" &gt;::<span data-count="11">:</span> [
    "test_broadcast_add" &gt;:<span data-count="11">:</span> test_broadcast_add;
    "test_broadcast_mul" &gt;:<span data-count="11">:</span> test_broadcast_mul;
    "test_broadcast_sub" &gt;:<span data-count="11">:</span> test_broadcast_sub;
    "test_broadcast_div" &gt;:<span data-count="11">:</span> test_broadcast_div;
    "test_create_basic" &gt;:<span data-count="11">:</span> test_create_basic;
    "test_from_ndarray_requires_grad_false" &gt;:<span data-count="11">:</span> test_from_ndarray_requires_grad_false;
    "test_zeros_shape" &gt;:<span data-count="11">:</span> test_zeros_shape;
    "test_ones_shape" &gt;:<span data-count="11">:</span> test_ones_shape;
    "test_rand_shape" &gt;:<span data-count="11">:</span> test_rand_shape;
    "test_xavier_init_shape" &gt;:<span data-count="11">:</span> test_xavier_init_shape;
    "test_he_init_shape" &gt;:<span data-count="11">:</span> test_he_init_shape;
    "test_get_set_values" &gt;:<span data-count="11">:</span> test_get_set_values;
    "test_add_grad_accumulate" &gt;:<span data-count="11">:</span> test_add_grad_accumulate;
    "test_matmul_non_square" &gt;:<span data-count="11">:</span> test_matmul_non_square;
    "test_reshape_mismatch" &gt;:<span data-count="11">:</span> test_reshape_mismatch;
    "test_sum_no_requires_grad" &gt;:<span data-count="11">:</span> test_sum_no_requires_grad;
    "test_sum_multiple_dims" &gt;:<span data-count="11">:</span> test_sum_multiple_dims;
    "test_mean_dim" &gt;:<span data-count="11">:</span> test_mean_dim;
    "test_neg_no_requires_grad" &gt;:<span data-count="11">:</span> test_neg_no_requires_grad;
    "test_relu_all_negative" &gt;:<span data-count="11">:</span> test_relu_all_negative;
    "test_softmax" &gt;:<span data-count="11">:</span> test_softmax;
    "test_log_softmax" &gt;:<span data-count="11">:</span> test_log_softmax;
    "test_slice" &gt;:<span data-count="11">:</span> test_slice;
  ]

let () =
  run_test_tt_mai<span data-count="1">n</span> suite
</code></pre>
      </div>
    </div>
    <script src="../../coverage.js"></script>
  </body>
</html>
